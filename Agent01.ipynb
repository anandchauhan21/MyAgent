{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9QIAPICDZmC"
      },
      "source": [
        "# Agent0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-JCj5t4OlI_"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "langchain \\\n",
        "langchain-community \\\n",
        "langchain-groq \\\n",
        "langchain-text-splitters \\\n",
        "chromadb \\\n",
        "duckduckgo-search \\\n",
        "sentence-transformers \\\n",
        "groq \\\n",
        "pyngrok \\\n",
        "streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "odAnIopTQJ6c"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mqQmc7T7P3zg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] =\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K17XGwxVQPvy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"db\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R2Gdz4CvQWav"
      },
      "outputs": [],
      "source": [
        "with open(\"data/notes.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "Artificial Intelligence is the simulation of human intelligence by machines.\n",
        "It includes machine learning, deep learning, and natural language processing.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65sluB7_QX7a"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "import os\n",
        "\n",
        "def load_docs():\n",
        "    docs = []\n",
        "    for file in os.listdir(\"data\"):\n",
        "        if file.endswith(\".txt\"):\n",
        "            loader = TextLoader(f\"data/{file}\")\n",
        "            docs.extend(loader.load())\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "def create_db():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(load_docs(), embeddings, persist_directory=\"db\")\n",
        "    db.persist()\n",
        "\n",
        "create_db()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KdnOCsQvQaCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039fa29d-4d12-4a83-c3cd-f4317b4e8a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2845828064.py:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  db = Chroma(\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "def web_search(q):\n",
        "    return \"\\n\".join([r[\"body\"] for r in DDGS().text(q, max_results=3)])\n",
        "\n",
        "db = Chroma(\n",
        "    persist_directory=\"db\",\n",
        "    embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        ")\n",
        "\n",
        "def doc_search(q):\n",
        "    results = db.similarity_search(q, k=3)\n",
        "    return \"\\n\".join([r.page_content for r in results])\n",
        "\n",
        "def ask(q):\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful AI study assistant.\n",
        "\n",
        "Web info:\n",
        "{web_search(q)}\n",
        "\n",
        "User notes:\n",
        "{doc_search(q)}\n",
        "\n",
        "Answer clearly:\n",
        "{q}\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QUL33i9QQcc0"
      },
      "outputs": [],
      "source": [
        "def create_plan(goal):\n",
        "    return llm.invoke(\n",
        "        f\"Create a simple, realistic 30-day study plan for: {goal}\"\n",
        "    ).content\n",
        "\n",
        "def reflect(topic):\n",
        "    return llm.invoke(\n",
        "        f\"Ask 5 deep questions and 3 new angles about: {topic}\"\n",
        "    ).content\n",
        "\n",
        "def challenge(idea):\n",
        "    return llm.invoke(\n",
        "        f\"Critically analyze this idea: {idea}\"\n",
        "    ).content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr_xn87mQgd0"
      },
      "outputs": [],
      "source": [
        "print(ask(\"Search best free Python course\"))\n",
        "print(ask(\"What is in my notes about AI?\"))\n",
        "print(create_plan(\"Learn Python\"))\n",
        "print(reflect(\"AI in education\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OfwRjrbS32k"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from __main__ import ask, create_plan, reflect, challenge\n",
        "\n",
        "st.set_page_config(page_title=\"AI Study Agent\")\n",
        "st.title(\"üß† AI Study & Research Agent\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\"Chat\", \"Study Plan\", \"Thinking\"])\n",
        "\n",
        "with tab1:\n",
        "    q = st.text_input(\"Ask me anything\")\n",
        "    if st.button(\"Ask\"):\n",
        "        st.write(ask(q))\n",
        "\n",
        "with tab2:\n",
        "    goal = st.text_input(\"Your study goal\")\n",
        "    if st.button(\"Create Plan\"):\n",
        "        st.write(create_plan(goal))\n",
        "\n",
        "with tab3:\n",
        "    text = st.text_area(\"Topic or idea\")\n",
        "    if st.button(\"Reflect\"):\n",
        "        st.write(reflect(text))\n",
        "    if st.button(\"Challenge\"):\n",
        "        st.write(challenge(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL-fzS9cVW1g"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"1g3JDCIN1SkvhSQk767MrGLc4hw_6WLLB6dHC9UX7JRevdUbr\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m57ljPQjS7Ig"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåç Your AI Agent is live at:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACzYk9MCWHGi"
      },
      "outputs": [],
      "source": [
        "%%writefile backend.py\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "def web_search(q):\n",
        "    return \"\\n\".join([r[\"body\"] for r in DDGS().text(q, max_results=3)])\n",
        "\n",
        "db = Chroma(\n",
        "    persist_directory=\"db\",\n",
        "    embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        ")\n",
        "\n",
        "def doc_search(q):\n",
        "    results = db.similarity_search(q, k=3)\n",
        "    return \"\\n\".join([r.page_content for r in results])\n",
        "\n",
        "def ask(q):\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful AI study assistant.\n",
        "\n",
        "Web info:\n",
        "{web_search(q)}\n",
        "\n",
        "User notes:\n",
        "{doc_search(q)}\n",
        "\n",
        "Answer clearly:\n",
        "{q}\n",
        "\"\"\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "def create_plan(goal):\n",
        "    return llm.invoke(\n",
        "        f\"Create a simple, realistic 30-day study plan for: {goal}\"\n",
        "    ).content\n",
        "\n",
        "def reflect(topic):\n",
        "    return llm.invoke(\n",
        "        f\"Ask 5 deep questions and 3 new angles about: {topic}\"\n",
        "    ).content\n",
        "\n",
        "def challenge(idea):\n",
        "    return llm.invoke(\n",
        "        f\"Critically analyze this idea: {idea}\"\n",
        "    ).content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0azFoIEzWLgR"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from backend import ask, create_plan, reflect, challenge\n",
        "\n",
        "st.set_page_config(page_title=\"AI Study Agent\")\n",
        "st.title(\"üß† AI Study & Research Agent\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\"Chat\", \"Study Plan\", \"Thinking\"])\n",
        "\n",
        "with tab1:\n",
        "    q = st.text_input(\"Ask me anything\")\n",
        "    if st.button(\"Ask\"):\n",
        "        st.write(ask(q))\n",
        "\n",
        "with tab2:\n",
        "    goal = st.text_input(\"Your study goal\")\n",
        "    if st.button(\"Create Plan\"):\n",
        "        st.write(create_plan(goal))\n",
        "\n",
        "with tab3:\n",
        "    text = st.text_area(\"Topic or idea\")\n",
        "    if st.button(\"Reflect\"):\n",
        "        st.write(reflect(text))\n",
        "    if st.button(\"Challenge\"):\n",
        "        st.write(challenge(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TQXOcD0WOBK"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåç Your AI Agent is live at:\", public_url)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}